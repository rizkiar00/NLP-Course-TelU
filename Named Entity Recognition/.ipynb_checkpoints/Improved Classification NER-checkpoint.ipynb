{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baca file train dan test\n",
    "train_lines = []\n",
    "test_lines = []\n",
    "with open('kalimat_POS_NE_train.txt', 'r') as f:\n",
    "    train_lines = f.readlines()\n",
    "with open('kalimat_POS_NE_test.txt', 'r') as f:\n",
    "    test_lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk konversi label NE\n",
    "def convert_label(raw_ne):\n",
    "    new_label = 0\n",
    "    if (raw_ne=='B-PER'): new_label = 1\n",
    "    if (raw_ne=='I-PER'): new_label = 2\n",
    "    if (raw_ne=='B-ORG'): new_label = 3\n",
    "    if (raw_ne=='I-ORG'): new_label = 4\n",
    "    if (raw_ne=='B-LOC'): new_label = 5\n",
    "    if (raw_ne=='I-LOC'): new_label = 6\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inisialisasi kode token dan postag\n",
    "token_dict = {}\n",
    "postag_dict = {}\n",
    "\n",
    "#added prefix dict\n",
    "prefix1_dict = {}\n",
    "prefix2_dict = {}\n",
    "prefix3_dict = {}\n",
    "\n",
    "#added suffix dict\n",
    "suffix1_dict = {}\n",
    "suffix2_dict = {}\n",
    "suffix3_dict = {}\n",
    "\n",
    "#added postag dict\n",
    "postag_dict = {}\n",
    "\n",
    "counter_token = 0\n",
    "counter_postag = 0\n",
    "\n",
    "#added counter\n",
    "counter_prefix1 = 0\n",
    "counter_prefix2 = 0\n",
    "counter_prefix3 = 0\n",
    "counter_suffix1 = 0\n",
    "counter_suffix2 = 0\n",
    "counter_suffix3 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baca data token, postag, dan label NE\n",
    "train_sents = []\n",
    "test_sents = []\n",
    "sent = []\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sent = \n",
      "[('Kegiatan', 'NN', 0), ('Rapat', 'NN', 0), ('Pimpinan', 'NN', 0), ('Nasional', 'JJ', 0), ('dibuka', 'VBT', 0), ('oleh', 'IN', 0), ('Ketua', 'NN', 0), ('Umum', 'JJ', 0), ('AMI', 'NN', 3), ('pusat', 'NN', 0), (',', ',', 0), ('Putu', 'NNP', 1), ('Supadma', 'NNP', 2), ('Rudana', 'NNP', 2), (',', ',', 2), ('MBA', 'NNP', 2), (',', ',', 0), ('Kamis', 'NN', 0), (',', ',', 0), ('3', 'CDP', 0), ('Oktober', 'NNP', 0), ('tahun', 'NN', 0), ('2019', 'CDP', 0), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Menteri', 'NN', 0), ('Kelautan', 'NN', 0), ('dan', 'CC', 0), ('Perikanan', 'NN', 0), ('Susi', 'NNP', 1), ('Pudjiastuti', 'NNP', 2), ('meresmikan', 'VBT', 0), ('Cold', 'NN', 0), ('Storage', 'NN', 0), ('1.000', 'CDP', 0), ('ton', 'NN', 0), ('di', 'IN', 0), ('Penjaringan', 'NNP', 5), (',', ',', 0), ('Jakarta', 'NNP', 5), ('Utara', 'NN', 6), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Sri', 'NN', 1), ('Mulyani', 'NNP', 2), ('Indrawati', 'NNP', 2), ('berkesempatan', 'VBI', 0), ('untuk', 'IN', 0), ('meresmikan', 'VBT', 0), ('museum', 'NN', 0), ('dan', 'CC', 0), ('perpustakaan', 'NN', 0), ('baru', 'JJ', 0), ('milik', 'VBT', 0), ('Bea', 'NNP', 3), ('Cukai', 'NNP', 4), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Presiden', 'NN', 0), ('Jokowi', 'NNP', 1), ('akan', 'MD', 0), ('meresmikan', 'VBT', 0), ('jembatan', 'NN', 0), ('Holtekamp', 'NN', 0), ('di', 'IN', 0), ('Jayapura', 'NNP', 5), (',', ',', 6), ('Papua', 'NNP', 6), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Presiden', 'NN', 0), ('Joko', 'NNP', 1), ('Widodo', 'NNP', 2), ('meresmikan', 'NNP', 0), ('pengoperasian', 'NNP', 0), ('Palapa', 'NNP', 0), ('Ring', 'NNP', 0), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Menteri', 'NN', 0), ('Darmin', 'NN', 1), ('resmikan', 'NN', 0), ('Kawasan', 'NN', 5), ('Ekonomi', 'NN', 6), ('Khusus', 'JJ', 6), ('Sorong', 'NNP', 6), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Menteri', 'NN', 0), ('Pertahanan', 'NN', 0), ('Prabowo', 'NNP', 1), ('Subianto', 'NNP', 2), ('meresmikan', 'VBT', 0), ('patung', 'NN', 0), ('Panglima', 'NN', 1), ('Besar', 'JJ', 2), ('Jenderal', 'NN', 2), ('Sudirman', 'NNP', 2), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Menteri', 'NN', 0), ('PUPR', 'NN', 0), ('Basuki', 'NN', 1), ('Hadimuldjono', 'NNP', 2), ('mewakili', 'VBT', 0), ('Presiden', 'NN', 0), ('Joko', 'NNP', 1), ('Widodo', 'NNP', 2), ('meresmikan', 'VBT', 0), ('Rumah', 'NN', 0), ('Susun', 'NNP', 0), ('(', 'OP', 0), ('Rusun', 'NNP', 0), (')', 'CP', 0), ('di', 'IN', 0), ('lingkungan', 'NN', 0), ('Pondok', 'NNP', 3), ('Pesantren', 'NNP', 4), ('.', '.', 0)]\n",
      "train sent = \n",
      "[('Menteri', 'NN', 0), ('Agama', 'NN', 0), ('Republik', 'NN', 0), ('Indonesia', 'NNP', 0), (',', ',', 0), ('Jenderal', 'NN', 0), ('TNI', 'NNP', 0), ('(', 'OP', 0), ('Purn', 'NNP', 0), (')', 'CP', 0), ('Fachrul', 'NNP', 1), ('Razi', 'NNP', 2), ('meresmikan', 'VBT', 0), ('secara', 'IN', 0), ('simbolis', 'JJ', 0), ('penggunaan', 'NN', 0), ('tiga', 'CDP', 0), ('gedung', 'NN', 0), ('KUA', 'NN', 3), ('SBSN', 'NN', 4), ('di', 'IN', 0), ('Aceh', 'NNP', 5), ('.', '.', 0)]\n"
     ]
    }
   ],
   "source": [
    "# data train\n",
    "for line in train_lines:\n",
    "    line = line.rstrip('\\n')\n",
    "    curr_tuple = ()\n",
    "    if len(line)>1:\n",
    "        line_part = line.split(\" \")\n",
    "        t = (line_part[0], line_part[1], convert_label(line_part[2]))\n",
    "        if line_part[0].lower() not in token_dict.keys():\n",
    "            token_dict[line_part[0].lower()] = counter_token\n",
    "            counter_token = counter_token+1\n",
    "            \n",
    "            if len(line_part[0].lower())>=3:\n",
    "                #prefix\n",
    "                if line_part[0].lower()[0] not in prefix1_dict.keys():\n",
    "                    prefix1_dict[line_part[0].lower()[0]] = counter_prefix1\n",
    "                    counter_prefix1 = counter_prefix1+1\n",
    "                if line_part[0].lower()[0:2] not in prefix2_dict.keys():\n",
    "                    prefix2_dict[line_part[0].lower()[0:2]] = counter_prefix2\n",
    "                    counter_prefix2 = counter_prefix2+1\n",
    "                if line_part[0].lower()[0:3] not in prefix3_dict.keys():\n",
    "                    prefix3_dict[line_part[0].lower()[0:3]] = counter_prefix3\n",
    "                    counter_prefix3 = counter_prefix3+1\n",
    "            \n",
    "                #suffix\n",
    "                if line_part[0].lower()[-1] not in suffix1_dict.keys():\n",
    "                    suffix1_dict[line_part[0].lower()[-1]] = counter_suffix1\n",
    "                    counter_suffix1 = counter_suffix1+1\n",
    "                if line_part[0].lower()[-2:] not in suffix2_dict.keys():\n",
    "                    suffix2_dict[line_part[0].lower()[-2:]] = counter_suffix2\n",
    "                    counter_suffix2 = counter_suffix2+1\n",
    "                if line_part[0].lower()[-3:] not in suffix3_dict.keys():\n",
    "                    suffix3_dict[line_part[0].lower()[-3:]] = counter_suffix3\n",
    "                    counter_suffix3 = counter_suffix3+1\n",
    "            \n",
    "            elif len(line_part[0].lower())>=2:\n",
    "                #prefix\n",
    "                if line_part[0].lower()[0] not in prefix1_dict.keys():\n",
    "                    prefix1_dict[line_part[0].lower()[0]] = counter_prefix1\n",
    "                    counter_prefix1 = counter_prefix1+1\n",
    "                if line_part[0].lower()[0:2] not in prefix2_dict.keys():\n",
    "                    prefix2_dict[line_part[0].lower()[0:2]] = counter_prefix2\n",
    "                    counter_prefix2 = counter_prefix2+1\n",
    "                prefix3_dict[\"novalues\"] = 9999\n",
    "                \n",
    "                #suffix\n",
    "                if line_part[0].lower()[-1] not in suffix1_dict.keys():\n",
    "                    suffix1_dict[line_part[0].lower()[-1]] = counter_suffix1\n",
    "                    counter_suffix1 = counter_suffix1+1\n",
    "                if line_part[0].lower()[-2:] not in suffix2_dict.keys():\n",
    "                    suffix2_dict[line_part[0].lower()[-2:]] = counter_suffix2\n",
    "                    counter_suffix2 = counter_suffix2+1\n",
    "                suffix3_dict[\"novalues\"] = 9999\n",
    "            else:\n",
    "                #prefix\n",
    "                if line_part[0].lower()[0] not in prefix1_dict.keys():\n",
    "                    prefix1_dict[line_part[0].lower()[0]] = counter_prefix1\n",
    "                    counter_prefix1 = counter_prefix1+1\n",
    "                prefix2_dict[\"novalues\"] = 9999\n",
    "                prefix3_dict[\"novalues\"] = 9999\n",
    "                #suffix\n",
    "                if line_part[0].lower()[-1] not in suffix1_dict.keys():\n",
    "                    suffix1_dict[line_part[0].lower()[-1]] = counter_suffix1\n",
    "                    counter_suffix1 = counter_suffix1+1\n",
    "                suffix2_dict[\"novalues\"] = 9999\n",
    "                suffix3_dict[\"novalues\"] = 9999\n",
    "        if line_part[1] not in postag_dict.keys():\n",
    "            postag_dict[line_part[1]] = counter_postag\n",
    "            counter_postag = counter_postag+1\n",
    "        #print(t)\n",
    "        sent.append(t)\n",
    "    else:\n",
    "        print(\"train sent = \")\n",
    "        print(sent)\n",
    "        train_sents.append(sent)\n",
    "        sent = []\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sent = \n",
      "[('Wakil', 'NN', 0), ('Presiden', 'NN', 0), ('Jusuf', 'NN', 1), ('Kalla', 'NNP', 1), ('(', 'OP', 0), ('JK', 'NN', 0), (')', 'CP', 0), ('meresmikan', 'VBT', 0), ('Indonesian', 'NNP', 3), ('Agency', 'NN', 4), ('for', 'NN', 4), ('International', 'FW', 4), ('Development', 'NNP', 4), ('(', 'OP', 0), ('AID', 'NNP', 3), (')', 'CP', 0), ('atau', 'CC', 0), ('Lembaga', 'NN', 3), ('Kerja', 'NN', 4), ('Sama', 'JJ', 4), ('Pembangunan', 'NN', 4), ('Internasional', 'NNP', 4), ('Indonesia', 'NNP', 4), ('.', '.', 0), ('Menteri', 'NNP', 0), ('Energi', 'NNP', 0), ('dan', 'CC', 0), ('Sumber', 'NN', 0), ('Daya', 'NN', 0), ('Mineral', 'NNP', 0), ('(', 'OP', 0), ('ESDM', 'NNP', 0), (')', 'CP', 0), ('Arifin', 'NNP', 1), ('Tasrif', 'NNP', 2), ('meresmikan', 'VBT', 0), ('survei', 'NN', 0), ('seismik', 'NN', 0), ('untuk', 'IN', 0), ('mencari', 'VBT', 0), ('cadangan', 'NN', 0), ('minyak', 'NN', 0), ('dan', 'CC', 0), ('gas', 'NN', 0), ('(', 'OP', 0), ('migas)', 'NN', 0), ('.', '.', 0)]\n",
      "test sent = \n",
      "[('Menteri', 'NN', 0), ('PPN/Kepala', 'Kepala', 0), ('Bappenas', 'NNP', 0), (',', ',', 0), ('Bambang', 'NNP', 0), ('Brodjonegoro', 'NNP', 0), ('meresmikan', 'VBT', 0), ('Peluncuran', 'NN', 0), ('Sekolah', 'NN', 0), ('Terintegrasi', 'NN', 0), ('Berpola', 'NN', 0), ('Asrama', 'NN', 0), ('dan', 'CC', 0), ('Demonstrasi', 'NN', 0), ('.', '.', 0)]\n",
      "test sent = \n",
      "[('Menteri', 'NN', 0), ('ESDM', 'NNP', 0), ('Ignasius', 'NNP', 1), ('Jonan', 'NNP', 2), ('bersama', 'IN', 0), ('Gubernur', 'NN', 0), ('Kalimantan', 'NN', 0), ('Timur', 'NN', 0), ('Awang', 'NN', 1), ('Faroek', 'NNP', 2), ('saat', 'SC', 0), ('meresmikan', 'VBT', 0), ('Sumur', 'NNP', 0), ('Bor', 'NNP', 0), ('di', 'IN', 0), ('Penajam', 'NN', 5), ('.', '.', 0)]\n",
      "test sent = \n",
      "[('Menteri', 'NN', 0), ('Agama', 'NN', 0), ('RI', 'NNP', 0), (',', ',', 0), ('Bapak', 'NN', 0), ('Lukman', 'NN', 1), ('Hakim', 'NN', 2), ('Saifuddin', 'NNP', 2), (',', ',', 0), ('Kamis', 'NN', 0), ('(', 'OP', 0), ('10/5', '5', 0), (')', 'CP', 0), ('melakukan', 'VBT', 0), ('kunjungan', 'NN', 0), ('kerja', 'NN', 0), ('ke', 'IN', 0), ('IAIN', 'NN', 3), ('Kerinci', 'NN', 4), ('dalam', 'IN', 0), ('rangka', 'NN', 0), ('meresmikan', 'VBT', 0), ('alih', 'NN', 0), ('status', 'NN', 0), ('IAIN', 'NN', 3), ('Kerinci', 'NN', 4), ('dari', 'IN', 0), ('STAIN', 'NN', 3), ('Kerinci', 'NN', 4), (',', ',', 0), (\"Ma'had\", 'NNP', 3), ('Al', 'NNP', 4), ('-', '-', 4), (\"Jami'ah\", 'NNP', 4), ('dan', 'CC', 0), ('Seminar', 'NN', 0), ('Nasional', 'JJ', 0), ('.', '.', 0)]\n"
     ]
    }
   ],
   "source": [
    "# data test\n",
    "counter = 0\n",
    "for line in test_lines:\n",
    "    line = line.rstrip('\\n')\n",
    "    curr_tuple = ()\n",
    "    if len(line)>1:\n",
    "        line_part = line.split(\" \")\n",
    "        t = (line_part[0], line_part[1], convert_label(line_part[2]))\n",
    "        #print(t)\n",
    "        sent.append(t)\n",
    "    else:\n",
    "        print(\"test sent = \")\n",
    "        print(sent)\n",
    "        test_sents.append(sent)\n",
    "        sent = []\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kode untuk token/kata dan postag yang tidak muncul di data training, namun muncul di data testing\n",
    "token_dict['unk'] = 9999\n",
    "postag_dict['unk'] = 9999\n",
    "prefix1_dict['novalues'] = 9999\n",
    "prefix2_dict['novalues'] = 9999\n",
    "prefix3_dict['novalues'] = 9999\n",
    "suffix1_dict['novalues'] = 9999\n",
    "suffix2_dict['novalues'] = 9999\n",
    "suffix3_dict['novalues'] = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fungsi untuk ekstraksi fitur dari sebuah kalimat\n",
    "def word2features(sent, i):  \n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    if word.lower() not in token_dict.keys(): \n",
    "        word = 'unk'\n",
    "    if postag not in postag_dict.keys():\n",
    "        postag = 'unk'\n",
    "    if len(word) >= 3:\n",
    "        if word[0] not in prefix1_dict.keys():\n",
    "            prefix1 = \"novalues\"\n",
    "        else:\n",
    "            prefix1 = word[0]\n",
    "        if word[0:2] not in prefix2_dict.keys():\n",
    "            prefix2 = \"novalues\"\n",
    "        else:\n",
    "            prefix2 = word[0:2]\n",
    "        if word[0:3] not in prefix3_dict.keys():\n",
    "            prefix3 = \"novalues\"\n",
    "        else:\n",
    "            prefix3 = word[0:3]\n",
    "        if word[-1] not in suffix1_dict.keys():\n",
    "            suffix1 = \"novalues\"\n",
    "        else:\n",
    "            suffix1 = word[-1]\n",
    "        if word[-2:] not in suffix2_dict.keys():\n",
    "            suffix2 = \"novalues\"\n",
    "        else:\n",
    "            suffix2 = word[-2:]\n",
    "        if word[-3:] not in suffix3_dict.keys():\n",
    "            suffix3 = \"novalues\"\n",
    "        else:\n",
    "            suffix3 = word[-3:]\n",
    "    elif len(word) >=2:\n",
    "        if word[0] not in prefix1_dict.keys():\n",
    "            prefix1 = \"novalues\"\n",
    "        else:\n",
    "            prefix1 = word[0]\n",
    "        if word[0:2] not in prefix2_dict.keys():\n",
    "            prefix2 = \"novalues\"\n",
    "        else:\n",
    "            prefix2 = word[0:2]\n",
    "        prefix3 = \"novalues\"\n",
    "        if word[-1] not in suffix1_dict.keys():\n",
    "            suffix1 = \"novalues\"\n",
    "        else:\n",
    "            suffix1 = word[-1]\n",
    "        if word[-2:] not in suffix2_dict.keys():\n",
    "            suffix2 = \"novalues\"\n",
    "        else:\n",
    "            suffix2 = word[-2:]\n",
    "        suffix3 = \"novalues\"\n",
    "    else:\n",
    "        if word[0] not in prefix1_dict.keys():\n",
    "            prefix1 = \"novalues\"\n",
    "        else:\n",
    "            prefix1 = word[0]\n",
    "            prefix2 = \"novalues\"\n",
    "            prefix3 = \"novalues\"\n",
    "        if word[-1] not in suffix1_dict.keys():\n",
    "            suffix1 = \"novalues\"\n",
    "        else:\n",
    "            suffix1 = word[-1]\n",
    "            suffix2 = \"novalues\"\n",
    "            suffix3 = \"novalues\"\n",
    "    features = [\n",
    "        token_dict[word.lower()], # fitur kata dalam bentuk lowercase\n",
    "        word.isupper(), # fitur apakah karakter pertama token merupakan huruf kapital\n",
    "        word.istitle(), # fitur apakah token merupakan title\n",
    "        word.isdigit(), # fitur apakah token merupakan digit\n",
    "        prefix1_dict[prefix1], #fitur 1 karakter pertama token\n",
    "        prefix2_dict[prefix2], #fitur 2 karakter pertama token\n",
    "        prefix3_dict[prefix3], #fitur 3 karakter pertama token\n",
    "        suffix1_dict[suffix1], #fitur 1 karakter terakhir token\n",
    "        suffix2_dict[suffix2], #fitur 2 karakter terakhir token\n",
    "        suffix3_dict[suffix3], #fitur 3 karakter terakhir token\n",
    "        postag_dict[postag] # fitur kode postag token\n",
    "    ]\n",
    "                \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postag dictonary\n",
      "{'NN': 0, 'JJ': 1, 'VBT': 2, 'IN': 3, ',': 4, 'NNP': 5, 'CDP': 6, '.': 7, 'CC': 8, 'VBI': 9, 'MD': 10, 'OP': 11, 'CP': 12, 'FW': 13, 'unk': 9999}\n",
      "token dictonary\n",
      "{'kegiatan': 0, 'rapat': 1, 'pimpinan': 2, 'nasional': 3, 'dibuka': 4, 'oleh': 5, 'ketua': 6, 'umum': 7, 'ami': 8, 'pusat': 9, ',': 10, 'putu': 11, 'supadma': 12, 'rudana': 13, 'mba': 14, 'kamis': 15, '3': 16, 'oktober': 17, 'tahun': 18, '2019': 19, '.': 20, 'menteri': 21, 'kelautan': 22, 'dan': 23, 'perikanan': 24, 'susi': 25, 'pudjiastuti': 26, 'meresmikan': 27, 'cold': 28, 'storage': 29, '1.000': 30, 'ton': 31, 'di': 32, 'penjaringan': 33, 'jakarta': 34, 'utara': 35, 'sri': 36, 'mulyani': 37, 'indrawati': 38, 'berkesempatan': 39, 'untuk': 40, 'museum': 41, 'perpustakaan': 42, 'baru': 43, 'milik': 44, 'bea': 45, 'cukai': 46, 'presiden': 47, 'jokowi': 48, 'akan': 49, 'jembatan': 50, 'holtekamp': 51, 'jayapura': 52, 'papua': 53, 'joko': 54, 'widodo': 55, 'pengoperasian': 56, 'palapa': 57, 'ring': 58, 'darmin': 59, 'resmikan': 60, 'kawasan': 61, 'ekonomi': 62, 'khusus': 63, 'sorong': 64, 'pertahanan': 65, 'prabowo': 66, 'subianto': 67, 'patung': 68, 'panglima': 69, 'besar': 70, 'jenderal': 71, 'sudirman': 72, 'pupr': 73, 'basuki': 74, 'hadimuldjono': 75, 'mewakili': 76, 'rumah': 77, 'susun': 78, '(': 79, 'rusun': 80, ')': 81, 'lingkungan': 82, 'pondok': 83, 'pesantren': 84, 'agama': 85, 'republik': 86, 'indonesia': 87, 'tni': 88, 'purn': 89, 'fachrul': 90, 'razi': 91, 'secara': 92, 'simbolis': 93, 'penggunaan': 94, 'tiga': 95, 'gedung': 96, 'kua': 97, 'sbsn': 98, 'aceh': 99, 'wakil': 100, 'jusuf': 101, 'kalla': 102, 'jk': 103, 'indonesian': 104, 'agency': 105, 'for': 106, 'international': 107, 'development': 108, 'aid': 109, 'atau': 110, 'lembaga': 111, 'kerja': 112, 'sama': 113, 'pembangunan': 114, 'internasional': 115, 'unk': 9999}\n"
     ]
    }
   ],
   "source": [
    "print('postag dictonary')\n",
    "print(postag_dict)\n",
    "print('token dictonary')\n",
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ekstraksi fitur data train\n",
    "X_train = []\n",
    "y_train = []\n",
    "for s in train_sents:\n",
    "    for i in range(len(s)):\n",
    "        X_train.append(word2features(s,i))\n",
    "        y_train.append(s[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ekstraksi fitur data test\n",
    "X_test = []\n",
    "y_test = []\n",
    "for s in test_sents:\n",
    "    for i in range(len(s)):\n",
    "        X_test.append(word2features(s,i))\n",
    "        y_test.append(s[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtest\n",
      "[ 100    0    1    0 9999 9999 9999    2   56   81    0]\n"
     ]
    }
   ],
   "source": [
    "# Coba test, satu kata\n",
    "print('xtest')\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(X_test[0].reshape(1,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil klasifikasi data test:\n",
      "[5 5 5 5 0 3 0 0 5 5 0 5 5 0 3 0 0 5 5 5 5 5 5 0 5 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 3 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 5 5 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 5 0]\n"
     ]
    }
   ],
   "source": [
    "# Coba test, keseluruhan data test\n",
    "print('hasil klasifikasi data test:')\n",
    "print(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akurasi :  62.39 %\n"
     ]
    }
   ],
   "source": [
    "print(\"akurasi : \", round(clf.score(X_test,y_test)*100,2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.87      0.82        83\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.00      0.00      0.00         5\n",
      "           3       0.33      0.14      0.20         7\n",
      "           4       0.00      0.00      0.00        15\n",
      "           5       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62       117\n",
      "   macro avg       0.18      0.17      0.17       117\n",
      "weighted avg       0.57      0.62      0.59       117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
